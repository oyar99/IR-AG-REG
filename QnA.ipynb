{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulatory Information Retrieval and Answer Generation (RIRAG)\n",
    "\n",
    "This notebook solves the following task of the Regulatory Information Retrieval and Answer Generation competition.\n",
    "\n",
    "_Using the question and the passages retrieved in Subtask 1 (See ObliQA.ipynb notebook), participants must generate a comprehensive, accurate, and coherent answer. This subtask emphasizes the ability to synthesize information from multiple sources and present it in a clear and logical manner, ensuring that the answer fully addresses the compliance and obligation requirements of the query._\n",
    "\n",
    "The notebook demonstrates how we can leverage _Retrieval Augmented Generation_ and _Large Language Models_ to synthesize the results obtained through the hybrid (lexical and semantic) search to provide an accurate and precise answer to help professionals navigate the regulatory content.\n",
    "\n",
    "**Note**: This notebook needs to be updated in some places depending on the model and the parameters for the test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio  # For asynchronous programming\n",
    "import json  # For working with JSON data\n",
    "import logging  # For logging events and debugging\n",
    "import os  # For interacting with the operating system\n",
    "import time  # For time-related functions\n",
    "from datetime import datetime, timedelta  # For handling dates and times\n",
    "from collections import defaultdict  # For specialized container data types\n",
    "from typing import List, Optional  # For type hinting and annotations\n",
    "from dataclasses import dataclass  # For creating data classes\n",
    "\n",
    "import backoff  # For exponential backoff retries with decorators\n",
    "from dotenv import load_dotenv  # For loading environment variables from .env files\n",
    "from groq import Groq  # For interacting with Groq API (machine learning/inference)\n",
    "from tqdm import tqdm  # For progress bars in loops\n",
    "from tqdm.asyncio import tqdm_asyncio  # For progress bars with asynchronous code\n",
    "import nest_asyncio  # For patching event loops to support nested asyncio calls\n",
    "from openai import AzureOpenAI  # For interacting with Azure's OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy RePASs repo to validate our results - ONLY RUN ONCE\n",
    "#!git clone https://github.com/RegNLP/RePASs.git && cd RePASs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load passages from disk\n",
    "ndocs = 40  # Number of regulatory documents to process\n",
    "passages = defaultdict(str) # List to store all passages extracted from the regulatory documents\n",
    "\n",
    "# Extract the passages in each document\n",
    "for i in range(1, ndocs + 1):\n",
    "    with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
    "        doc = json.load(f)  # Loads the contents of the JSON file\n",
    "        for psg in doc:  # Map each passageId to the actual content\n",
    "            passages[psg[\"ID\"]] = psg[\"Passage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_dict = defaultdict(list) # Maps a question to the relevant passage and its corresponding ranking score\n",
    "\n",
    "# Load the rankings file in memory\n",
    "with open('data/rankings_hybrid.trec', 'r') as f:\n",
    "    # File format: QuestionID Q0 DocumentID Rank Score Method\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        question_id = parts[0]\n",
    "        document_id = parts[2]\n",
    "        rank = int(parts[3])\n",
    "        score = float(parts[4])\n",
    "        rankings_dict[question_id].append({\n",
    "            'doc': document_id,\n",
    "            'score': score\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_passages(question_id: str, rankings_dict: dict = rankings_dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts the passages content that are relevant for answering the given question.\n",
    "    Given a valid question id, it returns at least one passage and up to 10 passages\n",
    "    that surpass a given relevance threshold.\n",
    "    \n",
    "    Args:\n",
    "        question_id: The question id for which we want to extract the relevant passages\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of passages that are relevant for answering the given question\n",
    "    \"\"\"\n",
    "    retrieved_passages = []\n",
    "    should_stop = False\n",
    "    \n",
    "    for i in range(len(rankings_dict[question_id])):\n",
    "        # If there was a significant difference in relevance between two passages, don't extract more passages\n",
    "        # If 10 passages have already been extracted, don't extract more\n",
    "        if should_stop or len(retrieved_passages) == 10:\n",
    "            break\n",
    "            \n",
    "        # If no passage has been extracted, extract at least one\n",
    "        if len(retrieved_passages) == 0:\n",
    "            retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "            continue\n",
    "                \n",
    "        # Check if there is a relevance difference between this and the next passage of more than 10%\n",
    "        if i < len(rankings_dict[question_id]) - 1 and rankings_dict[question_id][i][\"score\"] - rankings_dict[question_id][i+1][\"score\"] > 0.1:\n",
    "                should_stop = True\n",
    "\n",
    "        # Don't include passages with low relevance\n",
    "        if rankings_dict[question_id][i][\"score\"] < 0.72:\n",
    "            break\n",
    "\n",
    "        retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "        \n",
    "    # Extract the plain text\n",
    "    retrieved_passages = [passages[doc] for doc in retrieved_passages]\n",
    "    \n",
    "    return retrieved_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You are a regulatory compliance assistant. Provide a **complete**, **coherent**, and**correct** response to the given question by synthesizing the information from the provided passages. Your answer should **fully integrate all relevant obligations, practices, and insights**, and directlyaddress the question. The passages are presented in order of relevance, so **prioritize the informationaccordingly** and ensure consistency in your response, avoiding any contradictions. Additionally, reference**specific regulations and key compliance requirements** outlined in the regulatory content to support youranswer. **Do not use any extraneous or external knowledge** outside of the provided passages when craftingyour response.',\n",
       " 'Question: question\\n\\nPassage: passage\\n\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_prompt(question: str, relevant_passages: list[str], system_prompt: str = None) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Builds the prompt that will be used to synthesize the passages\n",
    "    \n",
    "    Args:\n",
    "        question: A well formed regulatory question\n",
    "        relevant_passages: A list of relevant passages that should help answer the question\n",
    "        system_prompt: Optional custom system prompt. If None, uses default regulatory compliance prompt\n",
    "    \n",
    "    Returns:\n",
    "        A tuple with both the system prompt that contains instructions on how to answer and\n",
    "        the user prompt that contains the actual question and passages\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default system prompt if none is provided\n",
    "    default_system_prompt = (\"You are a regulatory compliance assistant. Provide a **complete**, **coherent**, and\"\n",
    "    \"**correct** response to the given question by synthesizing the information from the provided passages. \"\n",
    "    \"Your answer should **fully integrate all relevant obligations, practices, and insights**, and directly\"\n",
    "    \"address the question. The passages are presented in order of relevance, so **prioritize the information\"\n",
    "    \"accordingly** and ensure consistency in your response, avoiding any contradictions. Additionally, reference\"\n",
    "    \"**specific regulations and key compliance requirements** outlined in the regulatory content to support your\"\n",
    "    \"answer. **Do not use any extraneous or external knowledge** outside of the provided passages when crafting\"\n",
    "    \"your response.\")\n",
    "    \n",
    "    \n",
    "    # Use provided system prompt or fall back to default\n",
    "    system_prompt = system_prompt if system_prompt is not None else default_system_prompt\n",
    "\n",
    "    user_prompt = f\"Question: {question}\\n\\n\"\n",
    "    for passage in relevant_passages:\n",
    "        user_prompt += f\"Passage: {passage}\\n\\n\"\n",
    "        \n",
    "    return (system_prompt, user_prompt)\n",
    "\n",
    "build_prompt(\"question\", [\"passage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI - Batch deployment\n",
    "\n",
    "We use Azure OpenAI batch deployment to synthesize the retrieved passages for each question using `gpt-35-turbo` and `gpt-4o-mini`.\n",
    "We leverage the batch API to send all queries at once. The response is retrieved offline from the Azure open AI portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables to handle access to the openAI API using secrets\n",
    "# Variables to be defined:\n",
    "# QNA_ENDPOINT_URL: Deployment endpoint for inference/chat completion\n",
    "# QNA_OPENAI_API_KEY: Key to access openAI API\n",
    "#\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queue_batch_summarization_job(jobs):\n",
    "    \"\"\"\n",
    "    Create a batch Job in Azure open AI to generate the answers for all questions with a single request\n",
    "    \"\"\"\n",
    "    endpoint = os.getenv('QNA_ENDPOINT_URL')\n",
    "    openAIKey = os.getenv('QNA_OPENAI_API_KEY')\n",
    "\n",
    "    if not endpoint:\n",
    "        raise ValueError(\"No se ha definido la variable de entorno QNA_ENDPOINT_URL\")\n",
    "\n",
    "    if not openAIKey:\n",
    "        raise ValueError(\"No se ha definido la variable de entorno QNA_OPENAI_API_KEY\")\n",
    "\n",
    "    openAI_client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=openAIKey,\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "\n",
    "    # File that can either be uploaded manually or programatically\n",
    "    file_name = \"data/batch_questions.jsonl\"\n",
    "\n",
    "    # Save file contents using json lines format\n",
    "    with open(file_name, 'w') as file:\n",
    "        for job in jobs:\n",
    "            file.write(json.dumps(job) + '\\n')\n",
    "\n",
    "    # Upload the file programatically\n",
    "    batch_file = openAI_client.files.create(\n",
    "      file=open(file_name, \"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "    \n",
    "    # Wait until the file upload is done\n",
    "    while True:\n",
    "        file = openAI_client.files.retrieve(batch_file.id)\n",
    "        if file.status == \"processed\" or file.status == \"error\":\n",
    "            break\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Trigger the batch job using the uploaded file\n",
    "    # Result should terminate in less than 24 hours\n",
    "    batch_job = openAI_client.batches.create(\n",
    "      input_file_id=batch_file.id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\"\n",
    "    )\n",
    "    \n",
    "    return batch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_few_shot = (\"As a regulatory compliance assistant. Provide a **complete**, **coherent**, and \"\n",
    "\"**correct** response to the given question by synthesizing the information from the provided passages. \"\n",
    "\"Your answer should **fully integrate all relevant obligations, practices, and insights**, and directly \"\n",
    "\"address the question. The passages are presented in order of relevance, so **prioritize the information \"\n",
    "\"accordingly** and ensure consistency in your response, avoiding any contradictions. Additionally, reference \"\n",
    "\"**specific regulations and key compliance requirements** outlined in the regulatory content to support your \"\n",
    "\"answer. **Do not use any extraneous or external knowledge** outside of the provided passages when crafting \"\n",
    "\"your response.\"\n",
    "\"/n/nHere are a few examples.\"\n",
    "\"/n/nQuestion: What specific areas of inventory and delivery infrastructure should be covered in the independent third-party audits to satisfy the requirements of COBS Rule 22.4.2(d)?\"\n",
    "\"/n/nPassage: REGULATORY REQUIREMENTS - SPOT COMMODITY ACTIVITIES\\nDelivery & Storage\\nWhen applying COBS Rule 22.4.2(d), an Authorised Person should have independent third party audits carried out at appropriate times, for the inventories and deliveries undertaken at the storage facility, as well as the facilities infrastructure itself.  Where necessary, further third-party audits will be required for the obligations of Accepted Spot Commodities, as outlined in paragraph 26 above.\\n\"\n",
    "\"/n/nPassage: REGULATORY REQUIREMENTS - SPOT COMMODITY ACTIVITIES\\nDelivery & Storage\\nPursuant to COBS Rule 22.4.1, a delivery and/or storage facility used by an Authorised Person can be operated from within ADGM or outside ADGM.  Specifically, for the purposes of COBS Rules 22.4.1, an Authorised Person will need to submit to the FSRA the details of how each delivery and storage facility that it proposes to use, whether located inside or outside ADGM, meets the requirements set out in Rule 22.4.2(a) to (e).\\n\"\n",
    "\"/n/nYour response should read:\"\n",
    "\"/n/nTo satisfy the requirements of COBS Rule 22.4.2(d) for independent third-party audits, an Authorised Person should ensure that the audits cover the inventories and deliveries undertaken at the storage facility, as well as the infrastructure of the facility itself. Additionally, if the Authorised Person deals with Accepted Spot Commodities, further third-party audits will be necessary to fulfill their obligations as outlined in paragraph 26. As per COBS Rule 22.4.1, the delivery and storage facility used by the Authorised Person can be located within or outside ADGM, and the Authorised Person must submit details to the FSRA on how each facility meets the requirements set out in Rule 22.4.2(a) to (e). Therefore, the independent third-party audits should cover the areas of inventory, delivery, and infrastructure of the storage facility, as well as any obligations related to Accepted Spot Commodities and compliance with the requirements set out in COBS Rule 22.4.2(a) to (e)\"\n",
    "\"/n/nQuestion: What percentage of the Insurer's Net Written Premium is used to determine the non-proportional reinsurance element?\"\n",
    "\"/n/nPassage: The non proportional reinsurance element is calculated as 52% of the Insurer's Net Written Premium\"\n",
    "\"/n/nYour response should read:\"\n",
    "\"/n/nThe non-proportional reinsurance element is determined by calculating 52% of the Insurer's Net Written Premium.\"\n",
    "\"/n/nQuestion: Who is responsible for ensuring compliance with the obligations that apply to the Reporting Entity of a Fund under the provisions of this chapter, unless explicitly stated otherwise?\"\n",
    "\"/n/nPassage: Where an obligation applies to a Reporting Entity of a Fund under a provision of this chapter, except where expressly provided otherwise, the Governing Body of the Listed Fund must ensure compliance with that obligation.\"\n",
    "\"/n/nYour response should read:\"\n",
    "\"/n/nThe responsibility for ensuring compliance with the obligations that apply to the Reporting Entity of a Fund under the provisions of this chapter lies with the Governing Body of the Listed Fund. This is explicitly stated in the passage, which indicates that unless otherwise specified, it is the Governing Body that must ensure adherence to these obligations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2786/2786 [00:00<00:00, 92037.29it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "\n",
    "# NOTE: Change the model accordingly\n",
    "model = \"gpt-35-turbo\"\n",
    "# model = \"gpt-4o\"\n",
    "\n",
    "# Load the test dataset\n",
    "with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
    "    data = json.load(f)  # Load the JSON file\n",
    "    \n",
    "    # For each question:\n",
    "    for e in tqdm(data):  # tqdm adds a progress bar\n",
    "        query = e['Question']  # Extract the actual question\n",
    "        question_id = e[\"QuestionID\"] # Extract the question id\n",
    "\n",
    "        retrieved_passages = extract_passages(question_id)\n",
    "\n",
    "        (system_prompt, user_prompt) = build_prompt(query, retrieved_passages, system_prompt_few_shot)\n",
    "        \n",
    "        jobs.append({\n",
    "            \"custom_id\": question_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                \"temperature\": 0,\n",
    "                \"frequency_penalty\": 0.0,\n",
    "                \"presence_penalty\": 0.0,\n",
    "                \"max_tokens\": 1000,\n",
    "            }\n",
    "        })\n",
    "        \n",
    "batch_job = queue_batch_summarization_job(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: At this point the result has been downloaded offline and uploaded to the data folder\n",
    "\n",
    "answers = []\n",
    "batch_job_output = \"batch_result_35.jsonl\"\n",
    "# batch_job_output = \"batch_result_4o.jsonl\"\n",
    "\n",
    "with open(f'data/{batch_job_output}') as f:\n",
    "    # Parse each line of the file as a JSON\n",
    "    results = [json.loads(line) for line in f]\n",
    "    \n",
    "    for result in results:\n",
    "        # For each result, create an entry in answers array to later create the output file\n",
    "        question_id = result[\"custom_id\"]\n",
    "        # Since this function is deterministic, we can just call it again\n",
    "        # Clearly, there's an optimization we can do to avoid calling this twice\n",
    "        retrieved_passages = extract_passages(question_id) \n",
    "        answer = result[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        answers.append({\n",
    "            \"QuestionID\": question_id,\n",
    "            \"RetrievedPassages\": retrieved_passages,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "\n",
    "output_answers = \"answers-GPT35Turbo.json\"\n",
    "# output_answers = \"answers-GPT4o.json\"\n",
    "        \n",
    "# Save the results as a JSON file\n",
    "with open(f'data/{output_answers}', \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq API - Regular deployment\n",
    "\n",
    "Third, we use Groq's API to synthesize the retrieved passages for each question using `llama-3.1-70b-versatile`. We leverage Groq's high-performance infrastructure to process queries with minimal latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure minimal logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "@dataclass\n",
    "class Question:\n",
    "    \"\"\"Represents a question with its ID and content\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class ProcessedAnswer:\n",
    "    \"\"\"Represents a processed answer with metadata\"\"\"\n",
    "    question_id: str\n",
    "    retrieved_passages: List[str]\n",
    "    answer: str\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class GroqRateLimiter:\n",
    "    def __init__(self):\n",
    "        # Rate limits\n",
    "        self.tokens_per_minute = 30000\n",
    "        self.requests_per_minute = 1000\n",
    "        self.requests_per_day = 50000\n",
    "        \n",
    "        # Window durations\n",
    "        self.minute_window = timedelta(minutes=1)\n",
    "        self.day_window = timedelta(days=1)\n",
    "        \n",
    "        # Track requests with timestamps and token counts\n",
    "        self.requests = []  # List of (timestamp, tokens) tuples\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def wait_for_tokens(self, tokens_needed: int):\n",
    "        async with self.lock:\n",
    "            while True:\n",
    "                now = datetime.now()\n",
    "                minute_start = now - self.minute_window\n",
    "                day_start = now - self.day_window\n",
    "                \n",
    "                # Remove expired requests\n",
    "                self.requests = [(ts, tokens) for ts, tokens in self.requests \n",
    "                               if ts > day_start]\n",
    "                \n",
    "                # Calculate current usage\n",
    "                minute_requests = [(ts, tokens) for ts, tokens in self.requests \n",
    "                                 if ts > minute_start]\n",
    "                \n",
    "                minute_request_count = len(minute_requests)\n",
    "                day_request_count = len(self.requests)\n",
    "                minute_token_usage = sum(tokens for _, tokens in minute_requests)\n",
    "                \n",
    "                # Check all limits\n",
    "                if (minute_token_usage + tokens_needed <= self.tokens_per_minute and\n",
    "                    minute_request_count < self.requests_per_minute and\n",
    "                    day_request_count < self.requests_per_day):\n",
    "                    # Add new request\n",
    "                    self.requests.append((now, tokens_needed))\n",
    "                    return True\n",
    "                \n",
    "                # Calculate wait time based on the most restrictive limit\n",
    "                wait_times = []\n",
    "                \n",
    "                # Token limit check\n",
    "                if minute_token_usage + tokens_needed > self.tokens_per_minute and minute_requests:\n",
    "                    oldest_in_minute = min(ts for ts, _ in minute_requests)\n",
    "                    wait_times.append((oldest_in_minute + self.minute_window - now).total_seconds())\n",
    "                \n",
    "                # Requests per minute check\n",
    "                if minute_request_count >= self.requests_per_minute and minute_requests:\n",
    "                    oldest_in_minute = min(ts for ts, _ in minute_requests)\n",
    "                    wait_times.append((oldest_in_minute + self.minute_window - now).total_seconds())\n",
    "                \n",
    "                # Requests per day check\n",
    "                if day_request_count >= self.requests_per_day and self.requests:\n",
    "                    oldest_in_day = min(ts for ts, _ in self.requests)\n",
    "                    wait_times.append((oldest_in_day + self.day_window - now).total_seconds())\n",
    "                \n",
    "                # Wait for the shortest required time\n",
    "                if wait_times:\n",
    "                    wait_time = max(0.1, min(wait_times))  # Ensure minimum wait of 0.1s\n",
    "                    print(f\"Rate limit reached. Waiting {wait_time:.2f} seconds...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    # If no wait times calculated but still hitting limits, wait a small amount\n",
    "                    await asyncio.sleep(0.1)\n",
    "\n",
    "class GroqProcessor:\n",
    "    \"\"\"Handles processing of questions using Groq's API with rate limiting\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.groq_client = self._initialize_groq()\n",
    "        self.rate_limiter = GroqRateLimiter()\n",
    "        \n",
    "    def _initialize_groq(self) -> Groq:\n",
    "        \"\"\"Initialize Groq client with error handling\"\"\"\n",
    "        groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "        if not groq_api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY environment variable is not defined\")\n",
    "        return Groq(api_key=groq_api_key)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_passages(question_id: str, passages: dict, rankings_dict: dict) -> list[str]:\n",
    "        \"\"\"Extract relevant passages for a question\"\"\"\n",
    "        retrieved_passages = []\n",
    "        should_stop = False\n",
    "        \n",
    "        for i in range(len(rankings_dict[question_id])):\n",
    "            if should_stop or len(retrieved_passages) == 10:\n",
    "                break\n",
    "                \n",
    "            if len(retrieved_passages) == 0:\n",
    "                retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "                continue\n",
    "                    \n",
    "            if i < len(rankings_dict[question_id]) - 1 and rankings_dict[question_id][i][\"score\"] - rankings_dict[question_id][i+1][\"score\"] > 0.1:\n",
    "                    should_stop = True\n",
    "\n",
    "            if rankings_dict[question_id][i][\"score\"] < 0.72:\n",
    "                break\n",
    "\n",
    "            retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "            \n",
    "        return [passages[doc] for doc in retrieved_passages]\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        Exception,\n",
    "        max_tries=5,\n",
    "        giveup=lambda e: \"rate limit\" not in str(e).lower()\n",
    "    )\n",
    "    async def process_question(self, question: str, passages: List[str]) -> str:\n",
    "        (system_prompt, user_prompt) = build_prompt(question, passages, system_prompt_few_shot)\n",
    "        \n",
    "        # Estimate tokens (rough approximation)\n",
    "        estimated_tokens = len(system_prompt.split()) + len(user_prompt.split()) + 800\n",
    "        \n",
    "        # Wait for available tokens\n",
    "        await self.rate_limiter.wait_for_tokens(estimated_tokens)\n",
    "        \n",
    "        try:\n",
    "            completion = await asyncio.to_thread(\n",
    "                self.groq_client.chat.completions.create,\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.25,\n",
    "                max_tokens=800,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                retry_after = self._extract_retry_after(str(e))\n",
    "                print(f\"Rate limit hit, waiting {retry_after} seconds...\")\n",
    "                await asyncio.sleep(retry_after + 1)\n",
    "                return await self.process_question(question, passages)\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_retry_after(error_message: str) -> float:\n",
    "        \"\"\"Extract retry-after time from error message\"\"\"\n",
    "        try:\n",
    "            if \"try again in\" in error_message:\n",
    "                time_str = error_message.split(\"try again in\")[1].split(\"s\")[0].strip()\n",
    "                if \"m\" in time_str:\n",
    "                    minutes, seconds = time_str.split(\"m\")\n",
    "                    return float(minutes) * 60 + float(seconds)\n",
    "                return float(time_str)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return 60  # Default to 60 seconds if we can't parse the time\n",
    "\n",
    "async def main():\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = GroqProcessor()\n",
    "    answers = []\n",
    "    \n",
    "    try:\n",
    "        # Load necessary data\n",
    "        print(\"Loading passages...\")\n",
    "        passages = {}\n",
    "        for i in range(1, 41):\n",
    "            with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
    "                doc = json.load(f)\n",
    "                for psg in doc:\n",
    "                    passages[psg[\"ID\"]] = psg[\"Passage\"]\n",
    "\n",
    "        print(\"Loading rankings...\")\n",
    "        rankings_dict = defaultdict(list)\n",
    "        with open('data/rankings_hybrid.trec', 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                rankings_dict[parts[0]].append({\n",
    "                    'doc': parts[2],\n",
    "                    'score': float(parts[4])\n",
    "                })\n",
    "\n",
    "        # Process questions\n",
    "        print(\"Processing questions...\")\n",
    "        with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
    "            questions = json.load(f)\n",
    "            \n",
    "            for q in tqdm_asyncio(questions):\n",
    "                try:\n",
    "                    retrieved_passages = processor.extract_passages(\n",
    "                        q[\"QuestionID\"], \n",
    "                        passages, \n",
    "                        rankings_dict\n",
    "                    )\n",
    "                    \n",
    "                    answer = await processor.process_question(\n",
    "                        q[\"Question\"], \n",
    "                        retrieved_passages\n",
    "                    )\n",
    "                    \n",
    "                    answers.append({\n",
    "                        \"QuestionID\": q[\"QuestionID\"],\n",
    "                        \"RetrievedPassages\": retrieved_passages,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "                    \n",
    "                    # Save progress every 10 questions\n",
    "                    if len(answers) % 10 == 0:\n",
    "                        with open(\"data/answers-llama3.1.json\", \"w\") as f:\n",
    "                            json.dump(answers, f, indent=2)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing question {q['QuestionID']}: {e}\")\n",
    "                    # Save progress on error\n",
    "                    if answers:\n",
    "                        with open(\"data/answers-llama3.1-partial.json\", \"w\") as f:\n",
    "                            json.dump(answers, f, indent=2)\n",
    "\n",
    "        # Save final results\n",
    "        print(\"Saving final results...\")\n",
    "        with open(\"data/answers-llama3.1.json\", \"w\") as f:\n",
    "            json.dump(answers, f, indent=2)\n",
    "            \n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error during processing: {e}\")\n",
    "        # Save partial results if available\n",
    "        if answers:\n",
    "            with open(\"data/answers-llama3.1-partial.json\", \"w\") as f:\n",
    "                json.dump(answers, f, indent=2)\n",
    "        raise\n",
    "\n",
    "# Run the processor\n",
    "nest_asyncio.apply()\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Evaluation\n",
    "\n",
    "To evaluate and compare the results obtained from our two different processing methods:\n",
    "\n",
    "1. Batch Azure OpenAI deployment with `GPT-35-Turbo` and `GPT-4o-Mini`\n",
    "2. Groq deployment with `Llama-3.1`\n",
    "\n",
    "Run the following scripts using the RePASs virtual environment to evaluate each model's performance. Make sure you have activated the correct environment before running these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script to evaluate the results. Results are placed in /RePASs/results/hybrid or /RePASs/results/hybrid-4o\n",
    "## These scripts must be run using the virtual env in RePASs\n",
    "\n",
    "#python scripts/evaluate_model.py --input_file ./../results/answers-GPT35Turbo.json --group_method_name hybrid-GPT35Turbo\n",
    "#python scripts/evaluate_model.py --input_file ./../results/answers-GPT4o.json --group_method_name hybrid-GPT4o\n",
    "#python scripts/evaluate_model.py --input_file ./../results/answers-llama3.1.json --group_method_name hybrid-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_dict_unseen = defaultdict(list) # Maps a question to the relevant passage and its corresponding ranking score\n",
    "\n",
    "# Load the rankings file in memory\n",
    "with open('data/rankings_hybrid_unseen_test.trec', 'r') as f:\n",
    "    # File format: QuestionID Q0 DocumentID Rank Score Method\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        question_id = parts[0]\n",
    "        document_id = parts[2]\n",
    "        rank = int(parts[3])\n",
    "        score = float(parts[4])\n",
    "        rankings_dict_unseen[question_id].append({\n",
    "            'doc': document_id,\n",
    "            'score': score\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 446/446 [00:00<00:00, 56871.05it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "\n",
    "# Load the test dataset\n",
    "with open(\"ObliQADataset/RIRAG_Unseen_Questions.json\") as f:\n",
    "    data = json.load(f)  # Load the JSON file\n",
    "    \n",
    "    # For each question:\n",
    "    for e in tqdm(data):  # tqdm adds a progress bar\n",
    "        query = e['Question']  # Extract the actual question\n",
    "        question_id = e[\"QuestionID\"] # Extract the question id\n",
    "\n",
    "        retrieved_passages = extract_passages(question_id, rankings_dict_unseen)\n",
    "\n",
    "        (system_prompt, user_prompt) = build_prompt(query, retrieved_passages, system_prompt_few_shot)\n",
    "        \n",
    "        # Use GPT-35-Turbo which showed the best performance during our testing using both default prompt and default prompt with few-shot\n",
    "        jobs.append({\n",
    "            \"custom_id\": question_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-35-turbo\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                \"temperature\": 0,\n",
    "                \"frequency_penalty\": 0.0,\n",
    "                \"presence_penalty\": 0.0,\n",
    "                \"max_tokens\": 1000,\n",
    "            }\n",
    "        })\n",
    "        \n",
    "batch_job = queue_batch_summarization_job(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 446/446 [00:00<00:00, 955881.24it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = {}\n",
    "\n",
    "# Load the test dataset\n",
    "with open(\"ObliQADataset/RIRAG_Unseen_Questions.json\") as f:\n",
    "    data = json.load(f)  # Load the JSON file\n",
    "    \n",
    "    # For each question:\n",
    "    for e in tqdm(data):  # tqdm adds a progress bar\n",
    "        query = e['Question']  # Extract the actual question\n",
    "        question_id = e[\"QuestionID\"] # Extract the question id\n",
    "        \n",
    "        questions[question_id] = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point the result has been downloaded offline and uploaded to the data folder\n",
    "answers = []\n",
    "\n",
    "with open(\"data/batch_result_unseen.jsonl\") as f:\n",
    "    # Parse each line of the file as a JSON\n",
    "    results = [json.loads(line) for line in f]\n",
    "    \n",
    "    for result in results:\n",
    "        # For each result, create an entry in answers array to later create the output file\n",
    "        question_id = result[\"custom_id\"]\n",
    "        # Since this function is deterministic, we can just call it again\n",
    "        # Clearly, there's an optimization we can do to avoid calling this twice\n",
    "        retrieved_passages = extract_passages(question_id, rankings_dict_unseen) \n",
    "        answer = result[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        answers.append({\n",
    "            \"QuestionID\": question_id,\n",
    "            \"Question\": questions[question_id],\n",
    "            \"RetrievedPassages\": retrieved_passages,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "        \n",
    "# Save the results as a JSON file\n",
    "with open(\"data/answers-unseen.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
